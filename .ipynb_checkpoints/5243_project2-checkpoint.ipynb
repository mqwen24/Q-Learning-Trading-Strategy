{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "652ae16c-5dc5-484f-b47e-76a92ee36924",
   "metadata": {},
   "source": [
    "# Technical and Fundamental analysis\n",
    "**Team Project 1**  \n",
    "*Date: 3/10/2025*  \n",
    "*Presented by: MuQing Wen, Jingyu Hu*  \n",
    "\n",
    "\n",
    "\n",
    "Step1: Each team needs to choose 1 company manually from each sector of S&P500.\n",
    "\n",
    "Step2: Please choose 5 indicators for technical analysis and at least 7 indicators for fundamental analysis (Indicators from what we learned in class) from quarterly financial statements.\n",
    "\n",
    "Step3: Please analyze one company in terms of fundamental and technical perspectives \n",
    "- Technical analysis: Daily based analysis\n",
    "- Fundamental analysis: Quarterly based analysis\n",
    "- Technical analysis and Fundamental analysis need to be interconnected in your analysis.\n",
    "- January 2018 to December 2022 (Covid period) & January 2007 to December 2010 (Economic crisis)\n",
    " To begin with, please download the daily stock data and quarterly & annual financial statements. \n",
    "\n",
    "Your PPT includes 1) Technical analysis 2) Fundamental analysis 3) Corresponding economic events 4) Overview of the company and S&P500 sector overview etc. with 25-35 slides. Slides needs to be explained in 15-20 minutes with key points (not reading all sentences).  \n",
    "\n",
    "Submission: 1) Python code (both code and its html file), 2) any imported data in the code, 3) all financial statements, and 4) PPT as a zip file (Filename format: TeamNUM_Firstname_Lastname_UID.zip ) Each team will give a presentation in the classroom on March 25th, 2025 (TUE). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c23c39d9-48a6-48a0-82c5-ad7c7c3941b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9191fd-4a1c-48c4-81d4-c15c374c1223",
   "metadata": {},
   "source": [
    "### Fetch S&P 500 tickers from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9da40f4d-8ec7-46ff-852c-d2f224da500a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching S&P 500 tickers and GICS information from Wikipedia...\n",
      "Loaded 503 S&P 500 tickers (expected ~503 as of early 2025).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Security</th>\n",
       "      <th>GICS Sector</th>\n",
       "      <th>GICS Sub-Industry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MMM</td>\n",
       "      <td>3M</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Industrial Conglomerates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AOS</td>\n",
       "      <td>A. O. Smith</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Building Products</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABT</td>\n",
       "      <td>Abbott Laboratories</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Health Care Equipment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBV</td>\n",
       "      <td>AbbVie</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Biotechnology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACN</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>IT Consulting &amp; Other Services</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol             Security             GICS Sector  \\\n",
       "0    MMM                   3M             Industrials   \n",
       "1    AOS          A. O. Smith             Industrials   \n",
       "2    ABT  Abbott Laboratories             Health Care   \n",
       "3   ABBV               AbbVie             Health Care   \n",
       "4    ACN            Accenture  Information Technology   \n",
       "\n",
       "                GICS Sub-Industry  \n",
       "0        Industrial Conglomerates  \n",
       "1               Building Products  \n",
       "2           Health Care Equipment  \n",
       "3                   Biotechnology  \n",
       "4  IT Consulting & Other Services  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"Fetching S&P 500 tickers and GICS information from Wikipedia...\")\n",
    "# Read the table from Wikipedia\n",
    "sp500_table = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0]\n",
    "\n",
    "# Extract relevant columns\n",
    "tickers = sp500_table['Symbol'].tolist()\n",
    "gics_sectors = sp500_table[['Symbol', 'Security', 'GICS Sector', 'GICS Sub-Industry']]\n",
    "\n",
    "\n",
    "print(f\"Loaded {len(tickers)} S&P 500 tickers (expected ~503 as of early 2025).\")\n",
    "\n",
    "# Display the first few rows of the sector and sub-industry data\n",
    "gics_sectors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb5e4167-37ae-40ba-ac05-0c65e84169f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GICS Sector</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Industrials</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Financials</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Information Technology</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Health Care</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Consumer Discretionary</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Consumer Staples</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Utilities</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Real Estate</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Materials</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Communication Services</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Energy</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               GICS Sector  Count\n",
       "0              Industrials     78\n",
       "1               Financials     73\n",
       "2   Information Technology     69\n",
       "3              Health Care     60\n",
       "4   Consumer Discretionary     51\n",
       "5         Consumer Staples     38\n",
       "6                Utilities     31\n",
       "7              Real Estate     31\n",
       "8                Materials     26\n",
       "9   Communication Services     23\n",
       "10                  Energy     23"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute counts of unique GICS Sectors and GICS Sub-Industries\n",
    "sector_counts = sp500_table['GICS Sector'].value_counts().reset_index()\n",
    "sector_counts.columns = ['GICS Sector', 'Count']\n",
    "\n",
    "sector_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed81650a-a6af-4056-a138-a756f401d98c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GICS Sub-Industry</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Health Care Equipment</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Electric Utilities</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Semiconductors</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Industrial Machinery &amp; Supplies &amp; Components</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aerospace &amp; Defense</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>Other Specialized REITs</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>Computer &amp; Electronics Retail</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>Multi-Sector Holdings</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>Metal, Glass &amp; Plastic Containers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>Timber REITs</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>127 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                GICS Sub-Industry  Count\n",
       "0                           Health Care Equipment     17\n",
       "1                              Electric Utilities     15\n",
       "2                                  Semiconductors     14\n",
       "3    Industrial Machinery & Supplies & Components     14\n",
       "4                             Aerospace & Defense     12\n",
       "..                                            ...    ...\n",
       "122                       Other Specialized REITs      1\n",
       "123                 Computer & Electronics Retail      1\n",
       "124                         Multi-Sector Holdings      1\n",
       "125             Metal, Glass & Plastic Containers      1\n",
       "126                                  Timber REITs      1\n",
       "\n",
       "[127 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "subindustry_counts = sp500_table['GICS Sub-Industry'].value_counts().reset_index()\n",
    "subindustry_counts.columns = ['GICS Sub-Industry', 'Count']\n",
    "\n",
    "display(subindustry_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0d3a97d-b497-4f32-8f2a-97bf510f7b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Information Technology Companies:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Security</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACN</td>\n",
       "      <td>Accenture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ADBE</td>\n",
       "      <td>Adobe Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AMD</td>\n",
       "      <td>Advanced Micro Devices</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>AKAM</td>\n",
       "      <td>Akamai Technologies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>APH</td>\n",
       "      <td>Amphenol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>TYL</td>\n",
       "      <td>Tyler Technologies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>VRSN</td>\n",
       "      <td>Verisign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>WDC</td>\n",
       "      <td>Western Digital</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>WDAY</td>\n",
       "      <td>Workday, Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>ZBRA</td>\n",
       "      <td>Zebra Technologies</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Symbol                Security\n",
       "4      ACN               Accenture\n",
       "5     ADBE              Adobe Inc.\n",
       "6      AMD  Advanced Micro Devices\n",
       "12    AKAM     Akamai Technologies\n",
       "33     APH                Amphenol\n",
       "..     ...                     ...\n",
       "453    TYL      Tyler Technologies\n",
       "468   VRSN                Verisign\n",
       "490    WDC         Western Digital\n",
       "495   WDAY           Workday, Inc.\n",
       "500   ZBRA      Zebra Technologies\n",
       "\n",
       "[69 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# List of tickers and company names for companies in the Information Technology sector\n",
    "it_companies = sp500_table.loc[sp500_table['GICS Sector'] == 'Information Technology', ['Symbol', 'Security']]\n",
    "print(\"\\nInformation Technology Companies:\")\n",
    "display(it_companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23c32197-b23b-4aac-9088-e9fe880b693a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ACN': 'Accenture',\n",
       " 'ADBE': 'Adobe Inc.',\n",
       " 'AMD': 'Advanced Micro Devices',\n",
       " 'AKAM': 'Akamai Technologies',\n",
       " 'APH': 'Amphenol',\n",
       " 'ADI': 'Analog Devices',\n",
       " 'ANSS': 'Ansys',\n",
       " 'AAPL': 'Apple Inc.',\n",
       " 'AMAT': 'Applied Materials',\n",
       " 'ANET': 'Arista Networks',\n",
       " 'ADSK': 'Autodesk',\n",
       " 'AVGO': 'Broadcom',\n",
       " 'CDNS': 'Cadence Design Systems',\n",
       " 'CDW': 'CDW Corporation',\n",
       " 'CSCO': 'Cisco',\n",
       " 'CTSH': 'Cognizant',\n",
       " 'GLW': 'Corning Inc.',\n",
       " 'CRWD': 'CrowdStrike',\n",
       " 'DELL': 'Dell Technologies',\n",
       " 'ENPH': 'Enphase Energy',\n",
       " 'EPAM': 'EPAM Systems',\n",
       " 'FFIV': 'F5, Inc.',\n",
       " 'FICO': 'Fair Isaac',\n",
       " 'FSLR': 'First Solar',\n",
       " 'FTNT': 'Fortinet',\n",
       " 'IT': 'Gartner',\n",
       " 'GEN': 'Gen Digital',\n",
       " 'GDDY': 'GoDaddy',\n",
       " 'HPE': 'Hewlett Packard Enterprise',\n",
       " 'HPQ': 'HP Inc.',\n",
       " 'IBM': 'IBM',\n",
       " 'INTC': 'Intel',\n",
       " 'INTU': 'Intuit',\n",
       " 'JBL': 'Jabil',\n",
       " 'JNPR': 'Juniper Networks',\n",
       " 'KEYS': 'Keysight Technologies',\n",
       " 'KLAC': 'KLA Corporation',\n",
       " 'LRCX': 'Lam Research',\n",
       " 'MCHP': 'Microchip Technology',\n",
       " 'MU': 'Micron Technology',\n",
       " 'MSFT': 'Microsoft',\n",
       " 'MPWR': 'Monolithic Power Systems',\n",
       " 'MSI': 'Motorola Solutions',\n",
       " 'NTAP': 'NetApp',\n",
       " 'NVDA': 'Nvidia',\n",
       " 'NXPI': 'NXP Semiconductors',\n",
       " 'ON': 'ON Semiconductor',\n",
       " 'ORCL': 'Oracle Corporation',\n",
       " 'PLTR': 'Palantir Technologies',\n",
       " 'PANW': 'Palo Alto Networks',\n",
       " 'PTC': 'PTC Inc.',\n",
       " 'QCOM': 'Qualcomm',\n",
       " 'ROP': 'Roper Technologies',\n",
       " 'CRM': 'Salesforce',\n",
       " 'STX': 'Seagate Technology',\n",
       " 'NOW': 'ServiceNow',\n",
       " 'SWKS': 'Skyworks Solutions',\n",
       " 'SMCI': 'Supermicro',\n",
       " 'SNPS': 'Synopsys',\n",
       " 'TEL': 'TE Connectivity',\n",
       " 'TDY': 'Teledyne Technologies',\n",
       " 'TER': 'Teradyne',\n",
       " 'TXN': 'Texas Instruments',\n",
       " 'TRMB': 'Trimble Inc.',\n",
       " 'TYL': 'Tyler Technologies',\n",
       " 'VRSN': 'Verisign',\n",
       " 'WDC': 'Western Digital',\n",
       " 'WDAY': 'Workday, Inc.',\n",
       " 'ZBRA': 'Zebra Technologies'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickers_to_company = it_companies.set_index(\"Symbol\")[\"Security\"].to_dict()\n",
    "tickers_to_company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72253fe4-e453-48c4-9fa9-341dccbaa2e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TYL', 'VRSN', 'WDC', 'WDAY', 'ZBRA']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it_tickers = it_companies['Symbol'].tolist()\n",
    "it_tickers[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddc5ebc1-47ad-4df5-9919-91a9f58a3bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it_tickers.append('^GSPC')\n",
    "# it_tickers[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde93027-f11f-44f2-a7d8-86ca6415807f",
   "metadata": {},
   "source": [
    "### Download stock data of IT companies \n",
    "- January 2018 to December 2022 (Covid period) & January 2007 to December 2010 (Economic crisis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52c9cf02-7836-4652-98bb-146c27ee0bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[                       0%                       ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading price data of the S&P 500 companies from 2018 to 2024 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  69 of 69 completed\n"
     ]
    }
   ],
   "source": [
    "print(\"Downloading price data of the S&P 500 companies from 2018 to 2024 ...\")\n",
    "data0 = yf.download(it_tickers, start='2018-01-01', end='2024-12-31', threads=True, auto_adjust=True)\n",
    "if data0.empty:\n",
    "    raise ValueError(\"No price data downloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54cb1fda-dd3a-47dc-839d-37f3ef96bf83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1760, 345)\n"
     ]
    }
   ],
   "source": [
    "# Create features\n",
    "print(data0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2513c9ab-fc8f-4b4b-a229-7c3a1b765afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n"
     ]
    }
   ],
   "source": [
    "unique_names = data0.columns.get_level_values(0).unique()\n",
    "print(unique_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5df6480a-65c0-4c9b-8c57-176530f82510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with missing values: ['CrowdStrike', 'Palantir Technologies']\n",
      "Columns with missing values: ['CrowdStrike', 'Palantir Technologies']\n",
      "Columns with missing values: ['CrowdStrike', 'Palantir Technologies']\n",
      "Columns with missing values: ['CrowdStrike', 'Palantir Technologies']\n",
      "Columns with missing values: ['CrowdStrike', 'Palantir Technologies']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def save_columns_to_csv(col_names, df, folder_name):\n",
    "    # Create the folder if it doesn't exist\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "    \n",
    "    # Iterate through each unique column name\n",
    "    for col in col_names:\n",
    "        if col in df.columns:  # Ensure the column exists in the DataFrame\n",
    "            df0 = df[col]\n",
    "            df1 = df0.rename(columns=tickers_to_company)\n",
    "            columns_with_nan = df1.columns[df1.isnull().any()].tolist()\n",
    "            print(\"Columns with missing values:\", columns_with_nan)\n",
    "            df_cleaned = df1.drop(columns=columns_with_nan)\n",
    "            df_cleaned.to_csv(f\"{folder_name}/{col}.csv\", index=True)\n",
    "        else:\n",
    "            print(f\"Warning: Column '{col}' not found in DataFrame.\")\n",
    "\n",
    "save_columns_to_csv(unique_names, data0, \"2018_to_2024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2aff79a-e532-4ac1-8fc2-649fbec87807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded DataFrames: ['Close', 'High', 'Low', 'Open', 'Volume']\n",
      "\n",
      "DataFrame: Close\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Apple Inc.</th>\n",
       "      <th>Accenture</th>\n",
       "      <th>Adobe Inc.</th>\n",
       "      <th>Analog Devices</th>\n",
       "      <th>Autodesk</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-02</th>\n",
       "      <td>40.479851</td>\n",
       "      <td>138.659622</td>\n",
       "      <td>177.699997</td>\n",
       "      <td>78.786263</td>\n",
       "      <td>107.120003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-03</th>\n",
       "      <td>40.472790</td>\n",
       "      <td>139.299561</td>\n",
       "      <td>181.039993</td>\n",
       "      <td>79.763664</td>\n",
       "      <td>109.379997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>40.660782</td>\n",
       "      <td>140.948975</td>\n",
       "      <td>183.220001</td>\n",
       "      <td>79.676407</td>\n",
       "      <td>112.070000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>41.123726</td>\n",
       "      <td>142.111710</td>\n",
       "      <td>185.339996</td>\n",
       "      <td>79.999306</td>\n",
       "      <td>110.839996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>40.970974</td>\n",
       "      <td>143.247391</td>\n",
       "      <td>185.039993</td>\n",
       "      <td>80.138916</td>\n",
       "      <td>111.419998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Apple Inc.   Accenture  Adobe Inc.  Analog Devices    Autodesk\n",
       "Date                                                                      \n",
       "2018-01-02   40.479851  138.659622  177.699997       78.786263  107.120003\n",
       "2018-01-03   40.472790  139.299561  181.039993       79.763664  109.379997\n",
       "2018-01-04   40.660782  140.948975  183.220001       79.676407  112.070000\n",
       "2018-01-05   41.123726  142.111710  185.339996       79.999306  110.839996\n",
       "2018-01-08   40.970974  143.247391  185.039993       80.138916  111.419998"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame: High\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Apple Inc.</th>\n",
       "      <th>Accenture</th>\n",
       "      <th>Adobe Inc.</th>\n",
       "      <th>Analog Devices</th>\n",
       "      <th>Autodesk</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-02</th>\n",
       "      <td>40.489252</td>\n",
       "      <td>138.893975</td>\n",
       "      <td>177.800003</td>\n",
       "      <td>79.109159</td>\n",
       "      <td>107.160004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-03</th>\n",
       "      <td>41.017975</td>\n",
       "      <td>139.696145</td>\n",
       "      <td>181.889999</td>\n",
       "      <td>79.946928</td>\n",
       "      <td>109.779999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>40.764179</td>\n",
       "      <td>141.381606</td>\n",
       "      <td>184.059998</td>\n",
       "      <td>80.531637</td>\n",
       "      <td>112.209999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>41.210672</td>\n",
       "      <td>142.156779</td>\n",
       "      <td>185.899994</td>\n",
       "      <td>80.522917</td>\n",
       "      <td>113.349998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>41.267063</td>\n",
       "      <td>143.319498</td>\n",
       "      <td>185.600006</td>\n",
       "      <td>80.470535</td>\n",
       "      <td>111.739998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Apple Inc.   Accenture  Adobe Inc.  Analog Devices    Autodesk\n",
       "Date                                                                      \n",
       "2018-01-02   40.489252  138.893975  177.800003       79.109159  107.160004\n",
       "2018-01-03   41.017975  139.696145  181.889999       79.946928  109.779999\n",
       "2018-01-04   40.764179  141.381606  184.059998       80.531637  112.209999\n",
       "2018-01-05   41.210672  142.156779  185.899994       80.522917  113.349998\n",
       "2018-01-08   41.267063  143.319498  185.600006       80.470535  111.739998"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame: Low\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Apple Inc.</th>\n",
       "      <th>Accenture</th>\n",
       "      <th>Adobe Inc.</th>\n",
       "      <th>Analog Devices</th>\n",
       "      <th>Autodesk</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-02</th>\n",
       "      <td>39.774873</td>\n",
       "      <td>137.704221</td>\n",
       "      <td>175.259995</td>\n",
       "      <td>77.608135</td>\n",
       "      <td>104.389999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-03</th>\n",
       "      <td>40.409344</td>\n",
       "      <td>137.893498</td>\n",
       "      <td>177.699997</td>\n",
       "      <td>78.585537</td>\n",
       "      <td>106.989998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>40.437540</td>\n",
       "      <td>139.497843</td>\n",
       "      <td>181.639999</td>\n",
       "      <td>79.632770</td>\n",
       "      <td>109.230003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>40.665491</td>\n",
       "      <td>140.723677</td>\n",
       "      <td>183.539993</td>\n",
       "      <td>79.266251</td>\n",
       "      <td>110.410004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>40.872274</td>\n",
       "      <td>141.363627</td>\n",
       "      <td>183.830002</td>\n",
       "      <td>79.728752</td>\n",
       "      <td>109.040001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Apple Inc.   Accenture  Adobe Inc.  Analog Devices    Autodesk\n",
       "Date                                                                      \n",
       "2018-01-02   39.774873  137.704221  175.259995       77.608135  104.389999\n",
       "2018-01-03   40.409344  137.893498  177.699997       78.585537  106.989998\n",
       "2018-01-04   40.437540  139.497843  181.639999       79.632770  109.230003\n",
       "2018-01-05   40.665491  140.723677  183.539993       79.266251  110.410004\n",
       "2018-01-08   40.872274  141.363627  183.830002       79.728752  109.040001"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame: Open\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Apple Inc.</th>\n",
       "      <th>Accenture</th>\n",
       "      <th>Adobe Inc.</th>\n",
       "      <th>Analog Devices</th>\n",
       "      <th>Autodesk</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-02</th>\n",
       "      <td>39.986368</td>\n",
       "      <td>138.353175</td>\n",
       "      <td>175.850006</td>\n",
       "      <td>77.878666</td>\n",
       "      <td>105.339996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-03</th>\n",
       "      <td>40.543288</td>\n",
       "      <td>137.893498</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>78.786252</td>\n",
       "      <td>107.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>40.545634</td>\n",
       "      <td>139.705144</td>\n",
       "      <td>181.929993</td>\n",
       "      <td>80.173839</td>\n",
       "      <td>110.129997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>40.757138</td>\n",
       "      <td>141.156309</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>79.868401</td>\n",
       "      <td>113.070000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>40.970974</td>\n",
       "      <td>141.841328</td>\n",
       "      <td>184.949997</td>\n",
       "      <td>80.121459</td>\n",
       "      <td>110.419998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Apple Inc.   Accenture  Adobe Inc.  Analog Devices    Autodesk\n",
       "Date                                                                      \n",
       "2018-01-02   39.986368  138.353175  175.850006       77.878666  105.339996\n",
       "2018-01-03   40.543288  137.893498  178.000000       78.786252  107.000000\n",
       "2018-01-04   40.545634  139.705144  181.929993       80.173839  110.129997\n",
       "2018-01-05   40.757138  141.156309  185.000000       79.868401  113.070000\n",
       "2018-01-08   40.970974  141.841328  184.949997       80.121459  110.419998"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame: Volume\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Apple Inc.</th>\n",
       "      <th>Accenture</th>\n",
       "      <th>Adobe Inc.</th>\n",
       "      <th>Analog Devices</th>\n",
       "      <th>Autodesk</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-02</th>\n",
       "      <td>102223600</td>\n",
       "      <td>3061900</td>\n",
       "      <td>2432800</td>\n",
       "      <td>2343200</td>\n",
       "      <td>2040600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-03</th>\n",
       "      <td>118071600</td>\n",
       "      <td>2064200</td>\n",
       "      <td>2561200</td>\n",
       "      <td>2009600</td>\n",
       "      <td>1953800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>89738400</td>\n",
       "      <td>1777000</td>\n",
       "      <td>2211400</td>\n",
       "      <td>1879600</td>\n",
       "      <td>2158700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>94640000</td>\n",
       "      <td>1597600</td>\n",
       "      <td>2376500</td>\n",
       "      <td>1799100</td>\n",
       "      <td>2384200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>82271200</td>\n",
       "      <td>2616900</td>\n",
       "      <td>2088000</td>\n",
       "      <td>1907400</td>\n",
       "      <td>1782100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Apple Inc.  Accenture  Adobe Inc.  Analog Devices  Autodesk\n",
       "Date                                                                   \n",
       "2018-01-02   102223600    3061900     2432800         2343200   2040600\n",
       "2018-01-03   118071600    2064200     2561200         2009600   1953800\n",
       "2018-01-04    89738400    1777000     2211400         1879600   2158700\n",
       "2018-01-05    94640000    1597600     2376500         1799100   2384200\n",
       "2018-01-08    82271200    2616900     2088000         1907400   1782100"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def load_csvs_to_dict(folder_name):\n",
    "    data_dict = {}\n",
    "    \n",
    "    # Ensure the folder exists\n",
    "    if not os.path.exists(folder_name):\n",
    "        print(f\"Error: Folder '{folder_name}' does not exist.\")\n",
    "        return data_dict\n",
    "\n",
    "    # Iterate through the files in the folder\n",
    "    for file in os.listdir(folder_name):\n",
    "        if file.endswith(\".csv\"):  # Ensure only CSV files are processed\n",
    "            file_path = os.path.join(folder_name, file)\n",
    "            df_name = os.path.splitext(file)[0]  # Extract name without extension\n",
    "            data_dict[df_name] = pd.read_csv(file_path, index_col=0)  # Read CSV with index\n",
    "            \n",
    "    return data_dict\n",
    "\n",
    "# Test Code\n",
    "folder = \"2018_to_2024\"\n",
    "data_frames1 = load_csvs_to_dict(folder)\n",
    "\n",
    "# Print the names of the DataFrames\n",
    "print(\"Loaded DataFrames:\", list(data_frames1.keys()))\n",
    "\n",
    "# Display the first 5 rows and first 10 columns of each DataFrame\n",
    "for name, df in data_frames1.items():\n",
    "    print(f\"\\nDataFrame: {name}\")\n",
    "    display(df.iloc[:5, :5])  # Display first 5 rows and first 5 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3227758-2540-4494-877c-ae7be94a91dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1259, 70)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Ticker</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>ACN</th>\n",
       "      <th>ADBE</th>\n",
       "      <th>ADI</th>\n",
       "      <th>ADSK</th>\n",
       "      <th>AKAM</th>\n",
       "      <th>AMAT</th>\n",
       "      <th>AMD</th>\n",
       "      <th>ANET</th>\n",
       "      <th>ANSS</th>\n",
       "      <th>...</th>\n",
       "      <th>TEL</th>\n",
       "      <th>TER</th>\n",
       "      <th>TRMB</th>\n",
       "      <th>TXN</th>\n",
       "      <th>TYL</th>\n",
       "      <th>VRSN</th>\n",
       "      <th>WDAY</th>\n",
       "      <th>WDC</th>\n",
       "      <th>ZBRA</th>\n",
       "      <th>^GSPC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-02</th>\n",
       "      <td>39.774858</td>\n",
       "      <td>137.704221</td>\n",
       "      <td>175.259995</td>\n",
       "      <td>77.608143</td>\n",
       "      <td>104.389999</td>\n",
       "      <td>64.699997</td>\n",
       "      <td>47.132257</td>\n",
       "      <td>10.34</td>\n",
       "      <td>14.172500</td>\n",
       "      <td>147.029999</td>\n",
       "      <td>...</td>\n",
       "      <td>83.438620</td>\n",
       "      <td>41.450215</td>\n",
       "      <td>40.590000</td>\n",
       "      <td>85.577321</td>\n",
       "      <td>176.929993</td>\n",
       "      <td>109.320000</td>\n",
       "      <td>100.239998</td>\n",
       "      <td>54.288378</td>\n",
       "      <td>102.750000</td>\n",
       "      <td>2682.360107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-03</th>\n",
       "      <td>40.409348</td>\n",
       "      <td>137.893483</td>\n",
       "      <td>177.699997</td>\n",
       "      <td>78.585567</td>\n",
       "      <td>106.989998</td>\n",
       "      <td>65.099998</td>\n",
       "      <td>48.945040</td>\n",
       "      <td>11.36</td>\n",
       "      <td>14.472500</td>\n",
       "      <td>148.350006</td>\n",
       "      <td>...</td>\n",
       "      <td>84.200050</td>\n",
       "      <td>41.844693</td>\n",
       "      <td>41.139999</td>\n",
       "      <td>86.478112</td>\n",
       "      <td>178.809998</td>\n",
       "      <td>108.550003</td>\n",
       "      <td>102.019997</td>\n",
       "      <td>55.809650</td>\n",
       "      <td>103.480003</td>\n",
       "      <td>2697.770020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>40.437536</td>\n",
       "      <td>139.497858</td>\n",
       "      <td>181.639999</td>\n",
       "      <td>79.632755</td>\n",
       "      <td>109.230003</td>\n",
       "      <td>65.440002</td>\n",
       "      <td>49.671980</td>\n",
       "      <td>11.97</td>\n",
       "      <td>14.463125</td>\n",
       "      <td>151.550003</td>\n",
       "      <td>...</td>\n",
       "      <td>85.740596</td>\n",
       "      <td>42.364268</td>\n",
       "      <td>42.259998</td>\n",
       "      <td>88.484468</td>\n",
       "      <td>180.820007</td>\n",
       "      <td>110.599998</td>\n",
       "      <td>107.309998</td>\n",
       "      <td>54.363419</td>\n",
       "      <td>105.839996</td>\n",
       "      <td>2719.070068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>40.665483</td>\n",
       "      <td>140.723647</td>\n",
       "      <td>183.539993</td>\n",
       "      <td>79.266236</td>\n",
       "      <td>110.410004</td>\n",
       "      <td>65.459999</td>\n",
       "      <td>49.892837</td>\n",
       "      <td>11.66</td>\n",
       "      <td>14.651250</td>\n",
       "      <td>151.919998</td>\n",
       "      <td>...</td>\n",
       "      <td>86.626017</td>\n",
       "      <td>42.970430</td>\n",
       "      <td>42.849998</td>\n",
       "      <td>88.681004</td>\n",
       "      <td>183.860001</td>\n",
       "      <td>111.730003</td>\n",
       "      <td>108.300003</td>\n",
       "      <td>55.673216</td>\n",
       "      <td>108.010002</td>\n",
       "      <td>2727.919922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>40.872278</td>\n",
       "      <td>141.363567</td>\n",
       "      <td>183.830002</td>\n",
       "      <td>79.728767</td>\n",
       "      <td>109.040001</td>\n",
       "      <td>65.209999</td>\n",
       "      <td>50.582985</td>\n",
       "      <td>11.85</td>\n",
       "      <td>14.906875</td>\n",
       "      <td>151.660004</td>\n",
       "      <td>...</td>\n",
       "      <td>87.484824</td>\n",
       "      <td>43.586221</td>\n",
       "      <td>42.700001</td>\n",
       "      <td>88.779290</td>\n",
       "      <td>182.919998</td>\n",
       "      <td>112.269997</td>\n",
       "      <td>108.110001</td>\n",
       "      <td>54.991029</td>\n",
       "      <td>109.570000</td>\n",
       "      <td>2737.600098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Ticker           AAPL         ACN        ADBE        ADI        ADSK  \\\n",
       "Date                                                                   \n",
       "2018-01-02  39.774858  137.704221  175.259995  77.608143  104.389999   \n",
       "2018-01-03  40.409348  137.893483  177.699997  78.585567  106.989998   \n",
       "2018-01-04  40.437536  139.497858  181.639999  79.632755  109.230003   \n",
       "2018-01-05  40.665483  140.723647  183.539993  79.266236  110.410004   \n",
       "2018-01-08  40.872278  141.363567  183.830002  79.728767  109.040001   \n",
       "\n",
       "Ticker           AKAM       AMAT    AMD       ANET        ANSS  ...  \\\n",
       "Date                                                            ...   \n",
       "2018-01-02  64.699997  47.132257  10.34  14.172500  147.029999  ...   \n",
       "2018-01-03  65.099998  48.945040  11.36  14.472500  148.350006  ...   \n",
       "2018-01-04  65.440002  49.671980  11.97  14.463125  151.550003  ...   \n",
       "2018-01-05  65.459999  49.892837  11.66  14.651250  151.919998  ...   \n",
       "2018-01-08  65.209999  50.582985  11.85  14.906875  151.660004  ...   \n",
       "\n",
       "Ticker            TEL        TER       TRMB        TXN         TYL  \\\n",
       "Date                                                                 \n",
       "2018-01-02  83.438620  41.450215  40.590000  85.577321  176.929993   \n",
       "2018-01-03  84.200050  41.844693  41.139999  86.478112  178.809998   \n",
       "2018-01-04  85.740596  42.364268  42.259998  88.484468  180.820007   \n",
       "2018-01-05  86.626017  42.970430  42.849998  88.681004  183.860001   \n",
       "2018-01-08  87.484824  43.586221  42.700001  88.779290  182.919998   \n",
       "\n",
       "Ticker            VRSN        WDAY        WDC        ZBRA        ^GSPC  \n",
       "Date                                                                    \n",
       "2018-01-02  109.320000  100.239998  54.288378  102.750000  2682.360107  \n",
       "2018-01-03  108.550003  102.019997  55.809650  103.480003  2697.770020  \n",
       "2018-01-04  110.599998  107.309998  54.363419  105.839996  2719.070068  \n",
       "2018-01-05  111.730003  108.300003  55.673216  108.010002  2727.919922  \n",
       "2018-01-08  112.269997  108.110001  54.991029  109.570000  2737.600098  \n",
       "\n",
       "[5 rows x 70 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_df = data0['Low']\n",
    "print(low_df.shape)\n",
    "low_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e36e680e-db85-42a0-b8ee-a94d16324bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1259, 70)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calculate_daily_returns(df):\n",
    "    daily_returns = df.pct_change()\n",
    "    return daily_returns\n",
    "\n",
    "daily_returns_df = calculate_daily_returns(close_df)\n",
    "daily_returns_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "882f7ea2-6dba-4cc3-98ed-c4ad94478655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with NaN values: 460\n"
     ]
    }
   ],
   "source": [
    "num_rows_with_nans = daily_returns_df.isna().any(axis=1).sum()\n",
    "print(\"Number of rows with NaN values:\", num_rows_with_nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "aef33a74-3118-41d3-bca2-ce889549e4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily return DataFrame for window size 5: (1528, 70)\n",
      "# rows with NaN values: 460\n",
      "Daily return DataFrame for window size 10: (1528, 70)\n",
      "# rows with NaN values: 460\n",
      "Daily return DataFrame for window size 20: (1528, 70)\n",
      "# rows with NaN values: 460\n",
      "Daily return DataFrame for window size 30: (1528, 70)\n",
      "# rows with NaN values: 460\n"
     ]
    }
   ],
   "source": [
    "def generate_daily_return_dfs(df, window_sizes):\n",
    "    return [calculate_daily_returns(df) for w in window_sizes]  # Calls daily return function for each window size\n",
    "\n",
    "# Example usage\n",
    "window_sizes = [5, 10, 20, 30]  # Define window sizes\n",
    "daily_returns_dfs = generate_daily_return_dfs(close_df, window_sizes)\n",
    "\n",
    "# Print the shape of each daily return DataFrame\n",
    "for i, w in enumerate(window_sizes):\n",
    "    print(f\"Daily return DataFrame for window size {w}: {daily_returns_dfs[i].shape}\")\n",
    "    num_rows_with_nans = daily_returns_df.isna().any(axis=1).sum()\n",
    "    print(\"# rows with NaN values:\", num_rows_with_nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "253d4ec6-fac0-446f-86cb-a207f0996a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2018-12-03', '2018-12-04', '2018-12-06', '2018-12-07',\n",
      "               '2018-12-10', '2018-12-11', '2018-12-12', '2018-12-13',\n",
      "               '2018-12-14', '2018-12-17',\n",
      "               ...\n",
      "               '2024-12-16', '2024-12-17', '2024-12-18', '2024-12-19',\n",
      "               '2024-12-20', '2024-12-23', '2024-12-24', '2024-12-26',\n",
      "               '2024-12-27', '2024-12-30'],\n",
      "              dtype='datetime64[ns]', name='Date', length=1528, freq=None)\n"
     ]
    }
   ],
   "source": [
    "def calculate_rolling_std(df, window=30):\n",
    "    rolling_std_df = df.rolling(window=window).std()  # Compute rolling std and fill NaNs with 0\n",
    "    return rolling_std_df\n",
    "\n",
    "# Example usage\n",
    "rolling_std_df = calculate_rolling_std(close_df, window=30)\n",
    "print(rolling_std_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "a52cc619-58f7-40d8-b489-7789fa9d51f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with NaN values: 488\n"
     ]
    }
   ],
   "source": [
    "num_rows_with_nans = rolling_std_df.isna().any(axis=1).sum()\n",
    "print(\"Number of rows with NaN values:\", num_rows_with_nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "b1713d53-7016-4ad5-8bbe-f2ffab133218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolling std DataFrame for window size 5: (1528, 70)\n",
      "# rows with NaN values: 463\n",
      "Rolling std DataFrame for window size 10: (1528, 70)\n",
      "# rows with NaN values: 468\n",
      "Rolling std DataFrame for window size 20: (1528, 70)\n",
      "# rows with NaN values: 478\n",
      "Rolling std DataFrame for window size 30: (1528, 70)\n",
      "# rows with NaN values: 488\n"
     ]
    }
   ],
   "source": [
    "def generate_rolling_std_dfs(df, window_sizes):\n",
    "    return [calculate_rolling_std(df, window=w) for w in window_sizes]\n",
    "\n",
    "# Example usage\n",
    "window_sizes = [5, 10, 20, 30]  # Define window sizes\n",
    "rolling_std_dfs = generate_rolling_std_dfs(close_df, window_sizes)\n",
    "\n",
    "# Print the shape of each rolling std DataFrame\n",
    "for i, w in enumerate(window_sizes):\n",
    "    print(f\"Rolling std DataFrame for window size {w}: {rolling_std_dfs[i].shape}\")\n",
    "    num_rows_with_nans = rolling_std_dfs[i].isna().any(axis=1).sum()\n",
    "    print(\"# rows with NaN values:\", num_rows_with_nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "04c20684-204a-405c-bfd3-d9ce5159daa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1528, 70)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calculate_sma(df, window=30):\n",
    "    sma_df = df.rolling(window=window).mean()  # Compute rolling mean and fill NaNs with 0\n",
    "    return sma_df\n",
    "\n",
    "# Example usage\n",
    "sma_df = calculate_sma(close_df, window=30)\n",
    "print(sma_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "8c90ceaf-231b-4a0b-8e5e-c9a3a81eb589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with NaN values: 488\n"
     ]
    }
   ],
   "source": [
    "num_rows_with_nans = sma_df.isna().any(axis=1).sum()\n",
    "print(\"Number of rows with NaN values:\", num_rows_with_nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "3e203e2f-057e-4465-aa1b-1c9cfe11344f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMA DataFrame for window size 5: (1528, 70)\n",
      "# rows with NaN values: 463\n",
      "SMA DataFrame for window size 10: (1528, 70)\n",
      "# rows with NaN values: 468\n",
      "SMA DataFrame for window size 20: (1528, 70)\n",
      "# rows with NaN values: 478\n",
      "SMA DataFrame for window size 30: (1528, 70)\n",
      "# rows with NaN values: 488\n"
     ]
    }
   ],
   "source": [
    "def generate_sma_dfs(df, window_sizes):\n",
    "    return [calculate_sma(df, window=w) for w in window_sizes]\n",
    "\n",
    "# Example usage\n",
    "window_sizes = [5, 10, 20, 30]  # Define window sizes\n",
    "sma_dfs = generate_sma_dfs(close_df, window_sizes)\n",
    "\n",
    "# Print the shape of each SMA DataFrame\n",
    "for i, w in enumerate(window_sizes):\n",
    "    print(f\"SMA DataFrame for window size {w}: {sma_dfs[i].shape}\")\n",
    "    num_rows_with_nans = sma_dfs[i].isna().any(axis=1).sum()\n",
    "    print(\"# rows with NaN values:\", num_rows_with_nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "a5b4dd8b-5630-4a0f-af5c-e03545768e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2018-12-03', '2018-12-04', '2018-12-06', '2018-12-07',\n",
      "               '2018-12-10', '2018-12-11', '2018-12-12', '2018-12-13',\n",
      "               '2018-12-14', '2018-12-17',\n",
      "               ...\n",
      "               '2024-12-16', '2024-12-17', '2024-12-18', '2024-12-19',\n",
      "               '2024-12-20', '2024-12-23', '2024-12-24', '2024-12-26',\n",
      "               '2024-12-27', '2024-12-30'],\n",
      "              dtype='datetime64[ns]', name='Date', length=1528, freq=None)\n",
      "Number of rows with NaN values: 459\n"
     ]
    }
   ],
   "source": [
    "def calculate_ema(df, span=30):\n",
    "    ema_df = df.ewm(span=span, adjust=False).mean()  # Compute EMA using exponential weighting\n",
    "    return ema_df\n",
    "\n",
    "ema_df = calculate_ema(close_df, span=30)\n",
    "print(ema_df.index)\n",
    "num_rows_with_nans = ema_df.isna().any(axis=1).sum()\n",
    "print(\"Number of rows with NaN values:\", num_rows_with_nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "85a742b7-f1b7-43fc-b875-c4d3904a69ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMA DataFrame for span size 5: (1528, 70)\n",
      "# rows with NaN values: 459\n",
      "EMA DataFrame for span size 10: (1528, 70)\n",
      "# rows with NaN values: 459\n",
      "EMA DataFrame for span size 20: (1528, 70)\n",
      "# rows with NaN values: 459\n",
      "EMA DataFrame for span size 30: (1528, 70)\n",
      "# rows with NaN values: 459\n"
     ]
    }
   ],
   "source": [
    "def generate_ema_dfs(df, window_sizes):\n",
    "    return [calculate_ema(df, span=w) for w in window_sizes]\n",
    "\n",
    "# Example usage\n",
    "window_sizes = [5, 10, 20, 30]  # Define window sizes\n",
    "ema_dfs = generate_ema_dfs(close_df, window_sizes)\n",
    "\n",
    "# Print the shape of each EMA DataFrame\n",
    "for i, w in enumerate(window_sizes):\n",
    "    print(f\"EMA DataFrame for span size {w}: {ema_dfs[i].shape}\")\n",
    "    num_rows_with_nans = ema_dfs[i].isna().any(axis=1).sum()\n",
    "    print(\"# rows with NaN values:\", num_rows_with_nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "d0507daf-1e6f-470c-9fd1-cc355a5a2be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame 1: Shape (1528, 70)\n",
      "DataFrame 2: Shape (1528, 70)\n",
      "DataFrame 3: Shape (1528, 70)\n",
      "DataFrame 4: Shape (1528, 70)\n",
      "DataFrame 5: Shape (1528, 70)\n",
      "DataFrame 6: Shape (1528, 70)\n",
      "DataFrame 7: Shape (1528, 70)\n",
      "DataFrame 8: Shape (1528, 70)\n",
      "DataFrame 9: Shape (1528, 70)\n",
      "DataFrame 10: Shape (1528, 70)\n",
      "DataFrame 11: Shape (1528, 70)\n",
      "DataFrame 12: Shape (1528, 70)\n",
      "DataFrame 13: Shape (1528, 70)\n",
      "DataFrame 14: Shape (1528, 70)\n",
      "DataFrame 15: Shape (1528, 70)\n",
      "DataFrame 16: Shape (1528, 70)\n"
     ]
    }
   ],
   "source": [
    "# Combine all DataFrames into a single list\n",
    "all_dfs = daily_returns_dfs + rolling_std_dfs + sma_dfs + ema_dfs\n",
    "\n",
    "# Print details for each DataFrame in the combined list\n",
    "for i, df in enumerate(all_dfs):\n",
    "    print(f\"DataFrame {i+1}: Shape {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f756875e-34bf-440c-8551-3dc3f621dba9",
   "metadata": {},
   "source": [
    "### Average True Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "6c355732-dd70-45c8-b21a-2666a04cdcea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1528, 70)\n",
      "Number of rows with NaN values: 472\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calculate_atr(close_df, high_df, low_df, window=14):\n",
    "    # Previous day's close\n",
    "    previous_close = close_df.shift(1)\n",
    "\n",
    "    # True Range (TR) calculation for each stock (column-wise per company)\n",
    "    tr = pd.DataFrame({\n",
    "        ticker: pd.concat([\n",
    "            (high_df[ticker] - low_df[ticker]),               # H - L\n",
    "            (high_df[ticker] - previous_close[ticker]).abs(), # |H - Cp|\n",
    "            (low_df[ticker] - previous_close[ticker]).abs()   # |L - Cp|\n",
    "        ], axis=1).max(axis=1)  # Max of the three per row, maintaining stock structure\n",
    "        for ticker in close_df.columns  # Iterate over each stock ticker\n",
    "    }, index=close_df.index)  # Keep original date index\n",
    "\n",
    "    # Calculate ATR using rolling mean\n",
    "    atr = tr.rolling(window=window).mean()\n",
    "\n",
    "    return atr\n",
    "\n",
    "# Example usage\n",
    "atr_df = calculate_atr(close_df, high_df, low_df, window=14)\n",
    "print(atr_df.shape)\n",
    "\n",
    "num_rows_with_nans = atr_df.isna().any(axis=1).sum()\n",
    "print(\"Number of rows with NaN values:\", num_rows_with_nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "848dc66b-b967-4ad9-9e6c-2eaed502d0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1528, 70)\n",
      "Number of rows with NaN values: 472\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calculate_k_line(close_df, high_df, low_df, window=14):\n",
    "    \"\"\"\n",
    "    Calculate the %K line (Fast Stochastic) for each stock.\n",
    "    \"\"\"\n",
    "    # Calculate rolling highest high and lowest low over the window\n",
    "    highest_high = high_df.rolling(window=window).max()\n",
    "    lowest_low = low_df.rolling(window=window).min()\n",
    "\n",
    "    # Compute %K Line\n",
    "    k_line = ((close_df - lowest_low) / (highest_high - lowest_low)) * 100\n",
    "\n",
    "    # Fill NaNs at the beginning (due to rolling window) with 0\n",
    "    return k_line\n",
    "\n",
    "# Example usage\n",
    "k_line_df = calculate_k_line(close_df, high_df, low_df, window=14)\n",
    "print(k_line_df.shape)\n",
    "num_rows_with_nans = k_line_df.isna().any(axis=1).sum()\n",
    "print(\"Number of rows with NaN values:\", num_rows_with_nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "3feb0b61-aa87-44d9-88ea-4865aaaaa452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1528, 70)\n",
      "Number of rows with NaN values: 474\n"
     ]
    }
   ],
   "source": [
    "def calculate_d_line(k_line_df, smooth_window=3):\n",
    "    \"\"\"\n",
    "    Calculate the %D line (Slow Stochastic) using SMA of %K.\n",
    "    \"\"\"\n",
    "    # Compute %D Line as SMA of %K\n",
    "    d_line = k_line_df.rolling(window=smooth_window).mean()\n",
    "\n",
    "    # Fill NaNs at the beginning with 0\n",
    "    return d_line\n",
    "\n",
    "# Example usage\n",
    "d_line_df = calculate_d_line(k_line_df, smooth_window=3)\n",
    "print(d_line_df.shape)\n",
    "num_rows_with_nans = d_line_df.isna().any(axis=1).sum()\n",
    "print(\"Number of rows with NaN values:\", num_rows_with_nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "0c87d2a5-04dd-4472-9eef-25525c214f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1528, 70)\n",
      "Number of rows with NaN values in RSI: 460\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calculate_rsi(return_df, window=14):\n",
    "    \"\"\"\n",
    "    Calculate the Relative Strength Index (RSI) for each stock.\n",
    "    \"\"\"\n",
    "    # Separate gains and losses\n",
    "    gains = return_df.where(return_df > 0, 0)  # Keep only positive returns\n",
    "    losses = -return_df.where(return_df < 0, 0)  # Convert negative returns to positive for averaging\n",
    "\n",
    "    # Compute moving averages of gains and losses\n",
    "    avg_gain = gains.rolling(window=window).mean()\n",
    "    avg_loss = losses.rolling(window=window).mean()\n",
    "\n",
    "    # Compute Relative Strength (RS)\n",
    "    rs = avg_gain / avg_loss\n",
    "\n",
    "    # Compute RSI\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "\n",
    "    return rsi\n",
    "\n",
    "# Example usage\n",
    "rsi_df = calculate_rsi(daily_returns_df, window=14)\n",
    "print(rsi_df.shape)\n",
    "\n",
    "# Count rows with NaN values\n",
    "num_rows_with_nans = rsi_df.isna().any(axis=1).sum()\n",
    "print(f\"Number of rows with NaN values in RSI: {num_rows_with_nans}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "ec3a78ed-92c3-4ba2-8d8a-f546e7a7092b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1528, 70)\n",
      "Number of rows with NaN values in RSI: 459\n"
     ]
    }
   ],
   "source": [
    "def calculate_ppo(close_df, short_window=12, long_window=26):\n",
    "    \"\"\"\n",
    "    Calculate the Percentage Price Oscillator (PPO) for each stock.\n",
    "    \"\"\"\n",
    "    short_ma = calculate_ema(close_df, span=short_window)  # Short-term EMA\n",
    "    long_ma = calculate_ema(close_df, span=long_window)  # Long-term EMA\n",
    "\n",
    "    ppo = ((short_ma - long_ma) / long_ma) * 100  # PPO formula\n",
    "    return ppo\n",
    "\n",
    "# Example usage\n",
    "ppo_df = calculate_ppo(close_df, short_window=12, long_window=26)\n",
    "print(ppo_df.shape)\n",
    "\n",
    "# Count rows with NaN values\n",
    "num_rows_with_nans = ppo_df.isna().any(axis=1).sum()\n",
    "print(f\"Number of rows with NaN values in RSI: {num_rows_with_nans}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "8c2e1df1-6fb2-4acd-98c6-e6297607fc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1528, 70)\n",
      "Number of rows with NaN values in RSI: 459\n"
     ]
    }
   ],
   "source": [
    "def calculate_ppo_signal(ppo_df, signal_window=9):\n",
    "    \"\"\"\n",
    "    Calculate the PPO signal line using a moving average of PPO.\n",
    "    \"\"\"\n",
    "    signal_line = calculate_ema(ppo_df, span=signal_window)  # EMA of PPO\n",
    "    return signal_line\n",
    "\n",
    "# Example usage\n",
    "ppo_signal_df = calculate_ppo_signal(ppo_df, signal_window=9)\n",
    "print(ppo_signal_df.shape)\n",
    "\n",
    "# Count rows with NaN values\n",
    "num_rows_with_nans = ppo_signal_df.isna().any(axis=1).sum()\n",
    "print(f\"Number of rows with NaN values in RSI: {num_rows_with_nans}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "761a3568-9bf7-4fb0-9428-18d0a6a69b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1528, 70)\n",
      "Number of rows with NaN values in RSI: 459\n"
     ]
    }
   ],
   "source": [
    "def calculate_ppo_histogram(ppo_df, ppo_signal_df):\n",
    "    \"\"\"\n",
    "    Calculate the PPO Histogram.\n",
    "    \"\"\"\n",
    "    ppo_histogram = ppo_df - ppo_signal_df  # Histogram = PPO - Signal Line\n",
    "    return ppo_histogram\n",
    "\n",
    "# Example usage\n",
    "ppo_histogram_df = calculate_ppo_histogram(ppo_df, ppo_signal_df)\n",
    "print(ppo_histogram_df.shape)\n",
    "\n",
    "# Count rows with NaN values\n",
    "num_rows_with_nans = ppo_histogram_df.isna().any(axis=1).sum()\n",
    "print(f\"Number of rows with NaN values in RSI: {num_rows_with_nans}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "87fb7cf9-e18d-4158-840e-603bf7605d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with NaN values in MACD: 459\n",
      "(1528, 70)\n"
     ]
    }
   ],
   "source": [
    "def calculate_macd(close_df, short_window=12, long_window=26):\n",
    "    \"\"\"\n",
    "    Calculate the MACD (Moving Average Convergence/Divergence) for each stock.\n",
    "    \"\"\"\n",
    "    short_ma = calculate_ema(close_df, span=short_window)  # Short-term EMA\n",
    "    long_ma = calculate_ema(close_df, span=long_window)  # Long-term EMA\n",
    "\n",
    "    macd = short_ma - long_ma  # MACD formula\n",
    "    return macd\n",
    "\n",
    "# Example usage\n",
    "macd_df = calculate_macd(close_df, short_window=12, long_window=26)\n",
    "\n",
    "# Print number of rows with NaN values\n",
    "num_rows_with_nans = macd_df.isna().any(axis=1).sum()\n",
    "print(f\"Number of rows with NaN values in MACD: {num_rows_with_nans}\")\n",
    "\n",
    "print(macd_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "29338f51-9f37-4741-a7e5-67c8d9fab023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with NaN values in Sharpe Ratio: 489\n",
      "(1528, 70)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calculate_sharpe_ratio(return_df, window=30, risk_free_rate=0):\n",
    "    \"\"\"\n",
    "    Calculate the Sharpe Ratio over a rolling window for each stock.\n",
    "    \"\"\"\n",
    "    # Compute rolling mean and standard deviation of returns\n",
    "    mean_return = return_df.rolling(window=window).mean()\n",
    "    std_return = return_df.rolling(window=window).std()\n",
    "\n",
    "    # Compute Sharpe Ratio\n",
    "    sharpe_ratio = (mean_return - risk_free_rate) / std_return\n",
    "\n",
    "    return sharpe_ratio\n",
    "\n",
    "# Example usage\n",
    "sharpe_ratio_df = calculate_sharpe_ratio(daily_returns_df, window=30)\n",
    "\n",
    "# Print number of rows with NaN values\n",
    "num_rows_with_nans = sharpe_ratio_df.isna().any(axis=1).sum()\n",
    "print(f\"Number of rows with NaN values in Sharpe Ratio: {num_rows_with_nans}\")\n",
    "\n",
    "print(sharpe_ratio_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "69138e69-11ef-4ee5-8874-e7d196192f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OBV Shape: (1528, 70)\n",
      "OBV Rows with NaN: 458\n",
      "VROC Shape: (1528, 70)\n",
      "VROC Rows with NaN: 473\n",
      "VWAP Shape: (1528, 70)\n",
      "VWAP Rows with NaN: 459\n",
      "MFI Shape: (1528, 70)\n",
      "MFI Rows with NaN: 460\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calculate_obv(close_df, volume_df):\n",
    "    \"\"\"\n",
    "    Calculate On-Balance Volume (OBV).\n",
    "    \"\"\"\n",
    "    obv = volume_df.copy()\n",
    "    obv.iloc[0] = 0  # Start OBV at zero\n",
    "    obv[close_df > close_df.shift(1)] = volume_df  # Add volume if price increases\n",
    "    obv[close_df < close_df.shift(1)] = -volume_df  # Subtract volume if price decreases\n",
    "    obv[close_df == close_df.shift(1)] = 0  # No change if price is the same\n",
    "    obv = obv.cumsum()\n",
    "    print(\"OBV Shape:\", obv.shape)\n",
    "    print(\"OBV Rows with NaN:\", obv.isna().any(axis=1).sum())\n",
    "    return obv\n",
    "\n",
    "def calculate_vroc(volume_df, window=14):\n",
    "    \"\"\"\n",
    "    Calculate Volume Rate of Change (VROC).\n",
    "    \"\"\"\n",
    "    vroc = ((volume_df - volume_df.shift(window)) / volume_df.shift(window)) * 100\n",
    "    print(\"VROC Shape:\", vroc.shape)\n",
    "    print(\"VROC Rows with NaN:\", vroc.isna().any(axis=1).sum())\n",
    "    return vroc\n",
    "\n",
    "def calculate_vwap(close_df, high_df, low_df, volume_df):\n",
    "    \"\"\"\n",
    "    Calculate Volume Weighted Average Price (VWAP).\n",
    "    \"\"\"\n",
    "    typical_price = (high_df + low_df + close_df) / 3\n",
    "    vwap = (typical_price * volume_df).cumsum() / volume_df.cumsum()\n",
    "    print(\"VWAP Shape:\", vwap.shape)\n",
    "    print(\"VWAP Rows with NaN:\", vwap.isna().any(axis=1).sum())\n",
    "    return vwap\n",
    "\n",
    "def calculate_mfi(close_df, high_df, low_df, volume_df, window=14):\n",
    "    \"\"\"\n",
    "    Calculate Money Flow Index (MFI).\n",
    "    \"\"\"\n",
    "    typical_price = (high_df + low_df + close_df) / 3\n",
    "    money_flow = typical_price * volume_df\n",
    "    positive_flow = money_flow.where(typical_price > typical_price.shift(1), 0)\n",
    "    negative_flow = money_flow.where(typical_price < typical_price.shift(1), 0)\n",
    "    money_flow_ratio = positive_flow.rolling(window=window).sum() / negative_flow.rolling(window=window).sum()\n",
    "    mfi = 100 - (100 / (1 + money_flow_ratio))\n",
    "    print(\"MFI Shape:\", mfi.shape)\n",
    "    print(\"MFI Rows with NaN:\", mfi.isna().any(axis=1).sum())\n",
    "    return mfi\n",
    "\n",
    "obv_df = calculate_obv(close_df, volume_df)\n",
    "vroc_df = calculate_vroc(volume_df, window=14)\n",
    "vwap_df = calculate_vwap(close_df, high_df, low_df, volume_df)\n",
    "mfi_df = calculate_mfi(close_df, high_df, low_df, volume_df, window=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842c85c7-e932-4813-9eb3-6757f3efaff4",
   "metadata": {},
   "source": [
    "### Prepare input for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "b63433b8-14a0-4aa5-8120-b8a8b0a3cef9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame 1: Shape (1528, 70)\n",
      "DataFrame 2: Shape (1528, 70)\n",
      "DataFrame 3: Shape (1528, 70)\n",
      "DataFrame 4: Shape (1528, 70)\n",
      "DataFrame 5: Shape (1528, 70)\n",
      "DataFrame 6: Shape (1528, 70)\n",
      "DataFrame 7: Shape (1528, 70)\n",
      "DataFrame 8: Shape (1528, 70)\n",
      "DataFrame 9: Shape (1528, 70)\n",
      "DataFrame 10: Shape (1528, 70)\n",
      "DataFrame 11: Shape (1528, 70)\n",
      "DataFrame 12: Shape (1528, 70)\n",
      "DataFrame 13: Shape (1528, 70)\n",
      "DataFrame 14: Shape (1528, 70)\n",
      "DataFrame 15: Shape (1528, 70)\n",
      "DataFrame 16: Shape (1528, 70)\n",
      "DataFrame 17: Shape (1528, 70)\n",
      "DataFrame 18: Shape (1528, 70)\n",
      "DataFrame 19: Shape (1528, 70)\n",
      "DataFrame 20: Shape (1528, 70)\n",
      "DataFrame 21: Shape (1528, 70)\n",
      "DataFrame 22: Shape (1528, 70)\n",
      "DataFrame 23: Shape (1528, 70)\n",
      "DataFrame 24: Shape (1528, 70)\n",
      "DataFrame 25: Shape (1528, 70)\n",
      "DataFrame 26: Shape (1528, 70)\n",
      "DataFrame 27: Shape (1528, 70)\n",
      "DataFrame 28: Shape (1528, 70)\n",
      "DataFrame 29: Shape (1528, 70)\n"
     ]
    }
   ],
   "source": [
    "all_dfs = [close_df, volume_df, high_df, low_df] + all_dfs + [rsi_df, atr_df, ppo_df, macd_df, sharpe_ratio_df, obv_df, vroc_df, vwap_df, mfi_df]\n",
    "\n",
    "# Print details for each DataFrame in the combined list\n",
    "for i, df in enumerate(all_dfs):\n",
    "    print(f\"DataFrame {i+1}: Shape {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "1de586e4-a02e-4a5b-b049-e2ece6cfe20f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows dropped: 489\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Timestamp('2018-12-03 00:00:00'),\n",
       " Timestamp('2018-12-04 00:00:00'),\n",
       " Timestamp('2018-12-06 00:00:00'),\n",
       " Timestamp('2018-12-07 00:00:00'),\n",
       " Timestamp('2018-12-10 00:00:00'),\n",
       " Timestamp('2018-12-11 00:00:00'),\n",
       " Timestamp('2018-12-12 00:00:00'),\n",
       " Timestamp('2018-12-13 00:00:00'),\n",
       " Timestamp('2018-12-14 00:00:00'),\n",
       " Timestamp('2018-12-17 00:00:00'),\n",
       " Timestamp('2018-12-18 00:00:00'),\n",
       " Timestamp('2018-12-19 00:00:00'),\n",
       " Timestamp('2018-12-20 00:00:00'),\n",
       " Timestamp('2018-12-21 00:00:00'),\n",
       " Timestamp('2018-12-24 00:00:00'),\n",
       " Timestamp('2018-12-26 00:00:00'),\n",
       " Timestamp('2018-12-27 00:00:00'),\n",
       " Timestamp('2018-12-28 00:00:00'),\n",
       " Timestamp('2018-12-31 00:00:00'),\n",
       " Timestamp('2019-01-02 00:00:00'),\n",
       " Timestamp('2019-01-03 00:00:00'),\n",
       " Timestamp('2019-01-04 00:00:00'),\n",
       " Timestamp('2019-01-07 00:00:00'),\n",
       " Timestamp('2019-01-08 00:00:00'),\n",
       " Timestamp('2019-01-09 00:00:00'),\n",
       " Timestamp('2019-01-10 00:00:00'),\n",
       " Timestamp('2019-01-11 00:00:00'),\n",
       " Timestamp('2019-01-14 00:00:00'),\n",
       " Timestamp('2019-01-15 00:00:00'),\n",
       " Timestamp('2019-01-16 00:00:00'),\n",
       " Timestamp('2019-01-17 00:00:00'),\n",
       " Timestamp('2019-01-18 00:00:00'),\n",
       " Timestamp('2019-01-22 00:00:00'),\n",
       " Timestamp('2019-01-23 00:00:00'),\n",
       " Timestamp('2019-01-24 00:00:00'),\n",
       " Timestamp('2019-01-25 00:00:00'),\n",
       " Timestamp('2019-01-28 00:00:00'),\n",
       " Timestamp('2019-01-29 00:00:00'),\n",
       " Timestamp('2019-01-30 00:00:00'),\n",
       " Timestamp('2019-01-31 00:00:00'),\n",
       " Timestamp('2019-02-01 00:00:00'),\n",
       " Timestamp('2019-02-04 00:00:00'),\n",
       " Timestamp('2019-02-05 00:00:00'),\n",
       " Timestamp('2019-02-06 00:00:00'),\n",
       " Timestamp('2019-02-07 00:00:00'),\n",
       " Timestamp('2019-02-08 00:00:00'),\n",
       " Timestamp('2019-02-11 00:00:00'),\n",
       " Timestamp('2019-02-12 00:00:00'),\n",
       " Timestamp('2019-02-13 00:00:00'),\n",
       " Timestamp('2019-02-14 00:00:00'),\n",
       " Timestamp('2019-02-15 00:00:00'),\n",
       " Timestamp('2019-02-19 00:00:00'),\n",
       " Timestamp('2019-02-20 00:00:00'),\n",
       " Timestamp('2019-02-21 00:00:00'),\n",
       " Timestamp('2019-02-22 00:00:00'),\n",
       " Timestamp('2019-02-25 00:00:00'),\n",
       " Timestamp('2019-02-26 00:00:00'),\n",
       " Timestamp('2019-02-27 00:00:00'),\n",
       " Timestamp('2019-02-28 00:00:00'),\n",
       " Timestamp('2019-03-01 00:00:00'),\n",
       " Timestamp('2019-03-04 00:00:00'),\n",
       " Timestamp('2019-03-05 00:00:00'),\n",
       " Timestamp('2019-03-06 00:00:00'),\n",
       " Timestamp('2019-03-07 00:00:00'),\n",
       " Timestamp('2019-03-08 00:00:00'),\n",
       " Timestamp('2019-03-11 00:00:00'),\n",
       " Timestamp('2019-03-12 00:00:00'),\n",
       " Timestamp('2019-03-13 00:00:00'),\n",
       " Timestamp('2019-03-14 00:00:00'),\n",
       " Timestamp('2019-03-15 00:00:00'),\n",
       " Timestamp('2019-03-18 00:00:00'),\n",
       " Timestamp('2019-03-19 00:00:00'),\n",
       " Timestamp('2019-03-20 00:00:00'),\n",
       " Timestamp('2019-03-21 00:00:00'),\n",
       " Timestamp('2019-03-22 00:00:00'),\n",
       " Timestamp('2019-03-25 00:00:00'),\n",
       " Timestamp('2019-03-26 00:00:00'),\n",
       " Timestamp('2019-03-27 00:00:00'),\n",
       " Timestamp('2019-03-28 00:00:00'),\n",
       " Timestamp('2019-03-29 00:00:00'),\n",
       " Timestamp('2019-04-01 00:00:00'),\n",
       " Timestamp('2019-04-02 00:00:00'),\n",
       " Timestamp('2019-04-03 00:00:00'),\n",
       " Timestamp('2019-04-04 00:00:00'),\n",
       " Timestamp('2019-04-05 00:00:00'),\n",
       " Timestamp('2019-04-08 00:00:00'),\n",
       " Timestamp('2019-04-09 00:00:00'),\n",
       " Timestamp('2019-04-10 00:00:00'),\n",
       " Timestamp('2019-04-11 00:00:00'),\n",
       " Timestamp('2019-04-12 00:00:00'),\n",
       " Timestamp('2019-04-15 00:00:00'),\n",
       " Timestamp('2019-04-16 00:00:00'),\n",
       " Timestamp('2019-04-17 00:00:00'),\n",
       " Timestamp('2019-04-18 00:00:00'),\n",
       " Timestamp('2019-04-22 00:00:00'),\n",
       " Timestamp('2019-04-23 00:00:00'),\n",
       " Timestamp('2019-04-24 00:00:00'),\n",
       " Timestamp('2019-04-25 00:00:00'),\n",
       " Timestamp('2019-04-26 00:00:00'),\n",
       " Timestamp('2019-04-29 00:00:00'),\n",
       " Timestamp('2019-04-30 00:00:00'),\n",
       " Timestamp('2019-05-01 00:00:00'),\n",
       " Timestamp('2019-05-02 00:00:00'),\n",
       " Timestamp('2019-05-03 00:00:00'),\n",
       " Timestamp('2019-05-06 00:00:00'),\n",
       " Timestamp('2019-05-07 00:00:00'),\n",
       " Timestamp('2019-05-08 00:00:00'),\n",
       " Timestamp('2019-05-09 00:00:00'),\n",
       " Timestamp('2019-05-10 00:00:00'),\n",
       " Timestamp('2019-05-13 00:00:00'),\n",
       " Timestamp('2019-05-14 00:00:00'),\n",
       " Timestamp('2019-05-15 00:00:00'),\n",
       " Timestamp('2019-05-16 00:00:00'),\n",
       " Timestamp('2019-05-17 00:00:00'),\n",
       " Timestamp('2019-05-20 00:00:00'),\n",
       " Timestamp('2019-05-21 00:00:00'),\n",
       " Timestamp('2019-05-22 00:00:00'),\n",
       " Timestamp('2019-05-23 00:00:00'),\n",
       " Timestamp('2019-05-24 00:00:00'),\n",
       " Timestamp('2019-05-28 00:00:00'),\n",
       " Timestamp('2019-05-29 00:00:00'),\n",
       " Timestamp('2019-05-30 00:00:00'),\n",
       " Timestamp('2019-05-31 00:00:00'),\n",
       " Timestamp('2019-06-03 00:00:00'),\n",
       " Timestamp('2019-06-04 00:00:00'),\n",
       " Timestamp('2019-06-05 00:00:00'),\n",
       " Timestamp('2019-06-06 00:00:00'),\n",
       " Timestamp('2019-06-07 00:00:00'),\n",
       " Timestamp('2019-06-10 00:00:00'),\n",
       " Timestamp('2019-06-11 00:00:00'),\n",
       " Timestamp('2019-06-12 00:00:00'),\n",
       " Timestamp('2019-06-13 00:00:00'),\n",
       " Timestamp('2019-06-14 00:00:00'),\n",
       " Timestamp('2019-06-17 00:00:00'),\n",
       " Timestamp('2019-06-18 00:00:00'),\n",
       " Timestamp('2019-06-19 00:00:00'),\n",
       " Timestamp('2019-06-20 00:00:00'),\n",
       " Timestamp('2019-06-21 00:00:00'),\n",
       " Timestamp('2019-06-24 00:00:00'),\n",
       " Timestamp('2019-06-25 00:00:00'),\n",
       " Timestamp('2019-06-26 00:00:00'),\n",
       " Timestamp('2019-06-27 00:00:00'),\n",
       " Timestamp('2019-06-28 00:00:00'),\n",
       " Timestamp('2019-07-01 00:00:00'),\n",
       " Timestamp('2019-07-02 00:00:00'),\n",
       " Timestamp('2019-07-03 00:00:00'),\n",
       " Timestamp('2019-07-05 00:00:00'),\n",
       " Timestamp('2019-07-08 00:00:00'),\n",
       " Timestamp('2019-07-09 00:00:00'),\n",
       " Timestamp('2019-07-10 00:00:00'),\n",
       " Timestamp('2019-07-11 00:00:00'),\n",
       " Timestamp('2019-07-12 00:00:00'),\n",
       " Timestamp('2019-07-15 00:00:00'),\n",
       " Timestamp('2019-07-16 00:00:00'),\n",
       " Timestamp('2019-07-17 00:00:00'),\n",
       " Timestamp('2019-07-18 00:00:00'),\n",
       " Timestamp('2019-07-19 00:00:00'),\n",
       " Timestamp('2019-07-22 00:00:00'),\n",
       " Timestamp('2019-07-23 00:00:00'),\n",
       " Timestamp('2019-07-24 00:00:00'),\n",
       " Timestamp('2019-07-25 00:00:00'),\n",
       " Timestamp('2019-07-26 00:00:00'),\n",
       " Timestamp('2019-07-29 00:00:00'),\n",
       " Timestamp('2019-07-30 00:00:00'),\n",
       " Timestamp('2019-07-31 00:00:00'),\n",
       " Timestamp('2019-08-01 00:00:00'),\n",
       " Timestamp('2019-08-02 00:00:00'),\n",
       " Timestamp('2019-08-05 00:00:00'),\n",
       " Timestamp('2019-08-06 00:00:00'),\n",
       " Timestamp('2019-08-07 00:00:00'),\n",
       " Timestamp('2019-08-08 00:00:00'),\n",
       " Timestamp('2019-08-09 00:00:00'),\n",
       " Timestamp('2019-08-12 00:00:00'),\n",
       " Timestamp('2019-08-13 00:00:00'),\n",
       " Timestamp('2019-08-14 00:00:00'),\n",
       " Timestamp('2019-08-15 00:00:00'),\n",
       " Timestamp('2019-08-16 00:00:00'),\n",
       " Timestamp('2019-08-19 00:00:00'),\n",
       " Timestamp('2019-08-20 00:00:00'),\n",
       " Timestamp('2019-08-21 00:00:00'),\n",
       " Timestamp('2019-08-22 00:00:00'),\n",
       " Timestamp('2019-08-23 00:00:00'),\n",
       " Timestamp('2019-08-26 00:00:00'),\n",
       " Timestamp('2019-08-27 00:00:00'),\n",
       " Timestamp('2019-08-28 00:00:00'),\n",
       " Timestamp('2019-08-29 00:00:00'),\n",
       " Timestamp('2019-08-30 00:00:00'),\n",
       " Timestamp('2019-09-03 00:00:00'),\n",
       " Timestamp('2019-09-04 00:00:00'),\n",
       " Timestamp('2019-09-05 00:00:00'),\n",
       " Timestamp('2019-09-06 00:00:00'),\n",
       " Timestamp('2019-09-09 00:00:00'),\n",
       " Timestamp('2019-09-10 00:00:00'),\n",
       " Timestamp('2019-09-11 00:00:00'),\n",
       " Timestamp('2019-09-12 00:00:00'),\n",
       " Timestamp('2019-09-13 00:00:00'),\n",
       " Timestamp('2019-09-16 00:00:00'),\n",
       " Timestamp('2019-09-17 00:00:00'),\n",
       " Timestamp('2019-09-18 00:00:00'),\n",
       " Timestamp('2019-09-19 00:00:00'),\n",
       " Timestamp('2019-09-20 00:00:00'),\n",
       " Timestamp('2019-09-23 00:00:00'),\n",
       " Timestamp('2019-09-24 00:00:00'),\n",
       " Timestamp('2019-09-25 00:00:00'),\n",
       " Timestamp('2019-09-26 00:00:00'),\n",
       " Timestamp('2019-09-27 00:00:00'),\n",
       " Timestamp('2019-09-30 00:00:00'),\n",
       " Timestamp('2019-10-01 00:00:00'),\n",
       " Timestamp('2019-10-02 00:00:00'),\n",
       " Timestamp('2019-10-03 00:00:00'),\n",
       " Timestamp('2019-10-04 00:00:00'),\n",
       " Timestamp('2019-10-07 00:00:00'),\n",
       " Timestamp('2019-10-08 00:00:00'),\n",
       " Timestamp('2019-10-09 00:00:00'),\n",
       " Timestamp('2019-10-10 00:00:00'),\n",
       " Timestamp('2019-10-11 00:00:00'),\n",
       " Timestamp('2019-10-14 00:00:00'),\n",
       " Timestamp('2019-10-15 00:00:00'),\n",
       " Timestamp('2019-10-16 00:00:00'),\n",
       " Timestamp('2019-10-17 00:00:00'),\n",
       " Timestamp('2019-10-18 00:00:00'),\n",
       " Timestamp('2019-10-21 00:00:00'),\n",
       " Timestamp('2019-10-22 00:00:00'),\n",
       " Timestamp('2019-10-23 00:00:00'),\n",
       " Timestamp('2019-10-24 00:00:00'),\n",
       " Timestamp('2019-10-25 00:00:00'),\n",
       " Timestamp('2019-10-28 00:00:00'),\n",
       " Timestamp('2019-10-29 00:00:00'),\n",
       " Timestamp('2019-10-30 00:00:00'),\n",
       " Timestamp('2019-10-31 00:00:00'),\n",
       " Timestamp('2019-11-01 00:00:00'),\n",
       " Timestamp('2019-11-04 00:00:00'),\n",
       " Timestamp('2019-11-05 00:00:00'),\n",
       " Timestamp('2019-11-06 00:00:00'),\n",
       " Timestamp('2019-11-07 00:00:00'),\n",
       " Timestamp('2019-11-08 00:00:00'),\n",
       " Timestamp('2019-11-11 00:00:00'),\n",
       " Timestamp('2019-11-12 00:00:00'),\n",
       " Timestamp('2019-11-13 00:00:00'),\n",
       " Timestamp('2019-11-14 00:00:00'),\n",
       " Timestamp('2019-11-15 00:00:00'),\n",
       " Timestamp('2019-11-18 00:00:00'),\n",
       " Timestamp('2019-11-19 00:00:00'),\n",
       " Timestamp('2019-11-20 00:00:00'),\n",
       " Timestamp('2019-11-21 00:00:00'),\n",
       " Timestamp('2019-11-22 00:00:00'),\n",
       " Timestamp('2019-11-25 00:00:00'),\n",
       " Timestamp('2019-11-26 00:00:00'),\n",
       " Timestamp('2019-11-27 00:00:00'),\n",
       " Timestamp('2019-11-29 00:00:00'),\n",
       " Timestamp('2019-12-02 00:00:00'),\n",
       " Timestamp('2019-12-03 00:00:00'),\n",
       " Timestamp('2019-12-04 00:00:00'),\n",
       " Timestamp('2019-12-05 00:00:00'),\n",
       " Timestamp('2019-12-06 00:00:00'),\n",
       " Timestamp('2019-12-09 00:00:00'),\n",
       " Timestamp('2019-12-10 00:00:00'),\n",
       " Timestamp('2019-12-11 00:00:00'),\n",
       " Timestamp('2019-12-12 00:00:00'),\n",
       " Timestamp('2019-12-13 00:00:00'),\n",
       " Timestamp('2019-12-16 00:00:00'),\n",
       " Timestamp('2019-12-17 00:00:00'),\n",
       " Timestamp('2019-12-18 00:00:00'),\n",
       " Timestamp('2019-12-19 00:00:00'),\n",
       " Timestamp('2019-12-20 00:00:00'),\n",
       " Timestamp('2019-12-23 00:00:00'),\n",
       " Timestamp('2019-12-24 00:00:00'),\n",
       " Timestamp('2019-12-26 00:00:00'),\n",
       " Timestamp('2019-12-27 00:00:00'),\n",
       " Timestamp('2019-12-30 00:00:00'),\n",
       " Timestamp('2019-12-31 00:00:00'),\n",
       " Timestamp('2020-01-02 00:00:00'),\n",
       " Timestamp('2020-01-03 00:00:00'),\n",
       " Timestamp('2020-01-06 00:00:00'),\n",
       " Timestamp('2020-01-07 00:00:00'),\n",
       " Timestamp('2020-01-08 00:00:00'),\n",
       " Timestamp('2020-01-09 00:00:00'),\n",
       " Timestamp('2020-01-10 00:00:00'),\n",
       " Timestamp('2020-01-13 00:00:00'),\n",
       " Timestamp('2020-01-14 00:00:00'),\n",
       " Timestamp('2020-01-15 00:00:00'),\n",
       " Timestamp('2020-01-16 00:00:00'),\n",
       " Timestamp('2020-01-17 00:00:00'),\n",
       " Timestamp('2020-01-21 00:00:00'),\n",
       " Timestamp('2020-01-22 00:00:00'),\n",
       " Timestamp('2020-01-23 00:00:00'),\n",
       " Timestamp('2020-01-24 00:00:00'),\n",
       " Timestamp('2020-01-27 00:00:00'),\n",
       " Timestamp('2020-01-28 00:00:00'),\n",
       " Timestamp('2020-01-29 00:00:00'),\n",
       " Timestamp('2020-01-30 00:00:00'),\n",
       " Timestamp('2020-01-31 00:00:00'),\n",
       " Timestamp('2020-02-03 00:00:00'),\n",
       " Timestamp('2020-02-04 00:00:00'),\n",
       " Timestamp('2020-02-05 00:00:00'),\n",
       " Timestamp('2020-02-06 00:00:00'),\n",
       " Timestamp('2020-02-07 00:00:00'),\n",
       " Timestamp('2020-02-10 00:00:00'),\n",
       " Timestamp('2020-02-11 00:00:00'),\n",
       " Timestamp('2020-02-12 00:00:00'),\n",
       " Timestamp('2020-02-13 00:00:00'),\n",
       " Timestamp('2020-02-14 00:00:00'),\n",
       " Timestamp('2020-02-18 00:00:00'),\n",
       " Timestamp('2020-02-19 00:00:00'),\n",
       " Timestamp('2020-02-20 00:00:00'),\n",
       " Timestamp('2020-02-21 00:00:00'),\n",
       " Timestamp('2020-02-24 00:00:00'),\n",
       " Timestamp('2020-02-25 00:00:00'),\n",
       " Timestamp('2020-02-26 00:00:00'),\n",
       " Timestamp('2020-02-27 00:00:00'),\n",
       " Timestamp('2020-02-28 00:00:00'),\n",
       " Timestamp('2020-03-02 00:00:00'),\n",
       " Timestamp('2020-03-03 00:00:00'),\n",
       " Timestamp('2020-03-04 00:00:00'),\n",
       " Timestamp('2020-03-05 00:00:00'),\n",
       " Timestamp('2020-03-06 00:00:00'),\n",
       " Timestamp('2020-03-09 00:00:00'),\n",
       " Timestamp('2020-03-10 00:00:00'),\n",
       " Timestamp('2020-03-11 00:00:00'),\n",
       " Timestamp('2020-03-12 00:00:00'),\n",
       " Timestamp('2020-03-13 00:00:00'),\n",
       " Timestamp('2020-03-16 00:00:00'),\n",
       " Timestamp('2020-03-17 00:00:00'),\n",
       " Timestamp('2020-03-18 00:00:00'),\n",
       " Timestamp('2020-03-19 00:00:00'),\n",
       " Timestamp('2020-03-20 00:00:00'),\n",
       " Timestamp('2020-03-23 00:00:00'),\n",
       " Timestamp('2020-03-24 00:00:00'),\n",
       " Timestamp('2020-03-25 00:00:00'),\n",
       " Timestamp('2020-03-26 00:00:00'),\n",
       " Timestamp('2020-03-27 00:00:00'),\n",
       " Timestamp('2020-03-30 00:00:00'),\n",
       " Timestamp('2020-03-31 00:00:00'),\n",
       " Timestamp('2020-04-01 00:00:00'),\n",
       " Timestamp('2020-04-02 00:00:00'),\n",
       " Timestamp('2020-04-03 00:00:00'),\n",
       " Timestamp('2020-04-06 00:00:00'),\n",
       " Timestamp('2020-04-07 00:00:00'),\n",
       " Timestamp('2020-04-08 00:00:00'),\n",
       " Timestamp('2020-04-09 00:00:00'),\n",
       " Timestamp('2020-04-13 00:00:00'),\n",
       " Timestamp('2020-04-14 00:00:00'),\n",
       " Timestamp('2020-04-15 00:00:00'),\n",
       " Timestamp('2020-04-16 00:00:00'),\n",
       " Timestamp('2020-04-17 00:00:00'),\n",
       " Timestamp('2020-04-20 00:00:00'),\n",
       " Timestamp('2020-04-21 00:00:00'),\n",
       " Timestamp('2020-04-22 00:00:00'),\n",
       " Timestamp('2020-04-23 00:00:00'),\n",
       " Timestamp('2020-04-24 00:00:00'),\n",
       " Timestamp('2020-04-27 00:00:00'),\n",
       " Timestamp('2020-04-28 00:00:00'),\n",
       " Timestamp('2020-04-29 00:00:00'),\n",
       " Timestamp('2020-04-30 00:00:00'),\n",
       " Timestamp('2020-05-01 00:00:00'),\n",
       " Timestamp('2020-05-04 00:00:00'),\n",
       " Timestamp('2020-05-05 00:00:00'),\n",
       " Timestamp('2020-05-06 00:00:00'),\n",
       " Timestamp('2020-05-07 00:00:00'),\n",
       " Timestamp('2020-05-08 00:00:00'),\n",
       " Timestamp('2020-05-11 00:00:00'),\n",
       " Timestamp('2020-05-12 00:00:00'),\n",
       " Timestamp('2020-05-13 00:00:00'),\n",
       " Timestamp('2020-05-14 00:00:00'),\n",
       " Timestamp('2020-05-15 00:00:00'),\n",
       " Timestamp('2020-05-18 00:00:00'),\n",
       " Timestamp('2020-05-19 00:00:00'),\n",
       " Timestamp('2020-05-20 00:00:00'),\n",
       " Timestamp('2020-05-21 00:00:00'),\n",
       " Timestamp('2020-05-22 00:00:00'),\n",
       " Timestamp('2020-05-26 00:00:00'),\n",
       " Timestamp('2020-05-27 00:00:00'),\n",
       " Timestamp('2020-05-28 00:00:00'),\n",
       " Timestamp('2020-05-29 00:00:00'),\n",
       " Timestamp('2020-06-01 00:00:00'),\n",
       " Timestamp('2020-06-02 00:00:00'),\n",
       " Timestamp('2020-06-03 00:00:00'),\n",
       " Timestamp('2020-06-04 00:00:00'),\n",
       " Timestamp('2020-06-05 00:00:00'),\n",
       " Timestamp('2020-06-08 00:00:00'),\n",
       " Timestamp('2020-06-09 00:00:00'),\n",
       " Timestamp('2020-06-10 00:00:00'),\n",
       " Timestamp('2020-06-11 00:00:00'),\n",
       " Timestamp('2020-06-12 00:00:00'),\n",
       " Timestamp('2020-06-15 00:00:00'),\n",
       " Timestamp('2020-06-16 00:00:00'),\n",
       " Timestamp('2020-06-17 00:00:00'),\n",
       " Timestamp('2020-06-18 00:00:00'),\n",
       " Timestamp('2020-06-19 00:00:00'),\n",
       " Timestamp('2020-06-22 00:00:00'),\n",
       " Timestamp('2020-06-23 00:00:00'),\n",
       " Timestamp('2020-06-24 00:00:00'),\n",
       " Timestamp('2020-06-25 00:00:00'),\n",
       " Timestamp('2020-06-26 00:00:00'),\n",
       " Timestamp('2020-06-29 00:00:00'),\n",
       " Timestamp('2020-06-30 00:00:00'),\n",
       " Timestamp('2020-07-01 00:00:00'),\n",
       " Timestamp('2020-07-02 00:00:00'),\n",
       " Timestamp('2020-07-06 00:00:00'),\n",
       " Timestamp('2020-07-07 00:00:00'),\n",
       " Timestamp('2020-07-08 00:00:00'),\n",
       " Timestamp('2020-07-09 00:00:00'),\n",
       " Timestamp('2020-07-10 00:00:00'),\n",
       " Timestamp('2020-07-13 00:00:00'),\n",
       " Timestamp('2020-07-14 00:00:00'),\n",
       " Timestamp('2020-07-15 00:00:00'),\n",
       " Timestamp('2020-07-16 00:00:00'),\n",
       " Timestamp('2020-07-17 00:00:00'),\n",
       " Timestamp('2020-07-20 00:00:00'),\n",
       " Timestamp('2020-07-21 00:00:00'),\n",
       " Timestamp('2020-07-22 00:00:00'),\n",
       " Timestamp('2020-07-23 00:00:00'),\n",
       " Timestamp('2020-07-24 00:00:00'),\n",
       " Timestamp('2020-07-27 00:00:00'),\n",
       " Timestamp('2020-07-28 00:00:00'),\n",
       " Timestamp('2020-07-29 00:00:00'),\n",
       " Timestamp('2020-07-30 00:00:00'),\n",
       " Timestamp('2020-07-31 00:00:00'),\n",
       " Timestamp('2020-08-03 00:00:00'),\n",
       " Timestamp('2020-08-04 00:00:00'),\n",
       " Timestamp('2020-08-05 00:00:00'),\n",
       " Timestamp('2020-08-06 00:00:00'),\n",
       " Timestamp('2020-08-07 00:00:00'),\n",
       " Timestamp('2020-08-10 00:00:00'),\n",
       " Timestamp('2020-08-11 00:00:00'),\n",
       " Timestamp('2020-08-12 00:00:00'),\n",
       " Timestamp('2020-08-13 00:00:00'),\n",
       " Timestamp('2020-08-14 00:00:00'),\n",
       " Timestamp('2020-08-17 00:00:00'),\n",
       " Timestamp('2020-08-18 00:00:00'),\n",
       " Timestamp('2020-08-19 00:00:00'),\n",
       " Timestamp('2020-08-20 00:00:00'),\n",
       " Timestamp('2020-08-21 00:00:00'),\n",
       " Timestamp('2020-08-24 00:00:00'),\n",
       " Timestamp('2020-08-25 00:00:00'),\n",
       " Timestamp('2020-08-26 00:00:00'),\n",
       " Timestamp('2020-08-27 00:00:00'),\n",
       " Timestamp('2020-08-28 00:00:00'),\n",
       " Timestamp('2020-08-31 00:00:00'),\n",
       " Timestamp('2020-09-01 00:00:00'),\n",
       " Timestamp('2020-09-02 00:00:00'),\n",
       " Timestamp('2020-09-03 00:00:00'),\n",
       " Timestamp('2020-09-04 00:00:00'),\n",
       " Timestamp('2020-09-08 00:00:00'),\n",
       " Timestamp('2020-09-09 00:00:00'),\n",
       " Timestamp('2020-09-10 00:00:00'),\n",
       " Timestamp('2020-09-11 00:00:00'),\n",
       " Timestamp('2020-09-14 00:00:00'),\n",
       " Timestamp('2020-09-15 00:00:00'),\n",
       " Timestamp('2020-09-16 00:00:00'),\n",
       " Timestamp('2020-09-17 00:00:00'),\n",
       " Timestamp('2020-09-18 00:00:00'),\n",
       " Timestamp('2020-09-21 00:00:00'),\n",
       " Timestamp('2020-09-22 00:00:00'),\n",
       " Timestamp('2020-09-23 00:00:00'),\n",
       " Timestamp('2020-09-24 00:00:00'),\n",
       " Timestamp('2020-09-25 00:00:00'),\n",
       " Timestamp('2020-09-28 00:00:00'),\n",
       " Timestamp('2020-09-29 00:00:00'),\n",
       " Timestamp('2020-09-30 00:00:00'),\n",
       " Timestamp('2020-10-01 00:00:00'),\n",
       " Timestamp('2020-10-02 00:00:00'),\n",
       " Timestamp('2020-10-05 00:00:00'),\n",
       " Timestamp('2020-10-06 00:00:00'),\n",
       " Timestamp('2020-10-07 00:00:00'),\n",
       " Timestamp('2020-10-08 00:00:00'),\n",
       " Timestamp('2020-10-09 00:00:00'),\n",
       " Timestamp('2020-10-12 00:00:00'),\n",
       " Timestamp('2020-10-13 00:00:00'),\n",
       " Timestamp('2020-10-14 00:00:00'),\n",
       " Timestamp('2020-10-15 00:00:00'),\n",
       " Timestamp('2020-10-16 00:00:00'),\n",
       " Timestamp('2020-10-19 00:00:00'),\n",
       " Timestamp('2020-10-20 00:00:00'),\n",
       " Timestamp('2020-10-21 00:00:00'),\n",
       " Timestamp('2020-10-22 00:00:00'),\n",
       " Timestamp('2020-10-23 00:00:00'),\n",
       " Timestamp('2020-10-26 00:00:00'),\n",
       " Timestamp('2020-10-27 00:00:00'),\n",
       " Timestamp('2020-10-28 00:00:00'),\n",
       " Timestamp('2020-10-29 00:00:00'),\n",
       " Timestamp('2020-10-30 00:00:00'),\n",
       " Timestamp('2020-11-02 00:00:00'),\n",
       " Timestamp('2020-11-03 00:00:00'),\n",
       " Timestamp('2020-11-04 00:00:00'),\n",
       " Timestamp('2020-11-05 00:00:00'),\n",
       " Timestamp('2020-11-06 00:00:00'),\n",
       " Timestamp('2020-11-09 00:00:00'),\n",
       " Timestamp('2020-11-10 00:00:00')]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def drop_rows_with_missing_values(df_list):\n",
    "    \n",
    "    combined_na_mask = pd.concat(df_list, axis=1).isna().any(axis=1)\n",
    "    dropped_rows = combined_na_mask[combined_na_mask].index.tolist()\n",
    "    \n",
    "    # Drop missing rows from all DataFrames\n",
    "    cleaned_dfs = [df.drop(index=dropped_rows) for df in df_list]\n",
    "    \n",
    "    print(f\"Number of rows dropped: {len(dropped_rows)}\")\n",
    "    \n",
    "    return cleaned_dfs, dropped_rows\n",
    "\n",
    "cleaned_dfs, dropped_dates = drop_rows_with_missing_values(all_dfs)\n",
    "dropped_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "d4e21b04-b4e8-405c-83c7-db48c91e9e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved df1 to stored_dfs/df1.csv\n",
      "Saved df2 to stored_dfs/df2.csv\n",
      "Saved df3 to stored_dfs/df3.csv\n",
      "Saved df4 to stored_dfs/df4.csv\n",
      "Saved df5 to stored_dfs/df5.csv\n",
      "Saved df6 to stored_dfs/df6.csv\n",
      "Saved df7 to stored_dfs/df7.csv\n",
      "Saved df8 to stored_dfs/df8.csv\n",
      "Saved df9 to stored_dfs/df9.csv\n",
      "Saved df10 to stored_dfs/df10.csv\n",
      "Saved df11 to stored_dfs/df11.csv\n",
      "Saved df12 to stored_dfs/df12.csv\n",
      "Saved df13 to stored_dfs/df13.csv\n",
      "Saved df14 to stored_dfs/df14.csv\n",
      "Saved df15 to stored_dfs/df15.csv\n",
      "Saved df16 to stored_dfs/df16.csv\n",
      "Saved df17 to stored_dfs/df17.csv\n",
      "Saved df18 to stored_dfs/df18.csv\n",
      "Saved df19 to stored_dfs/df19.csv\n",
      "Saved df20 to stored_dfs/df20.csv\n",
      "Saved df21 to stored_dfs/df21.csv\n",
      "Saved df22 to stored_dfs/df22.csv\n",
      "Saved df23 to stored_dfs/df23.csv\n",
      "Saved df24 to stored_dfs/df24.csv\n",
      "Saved df25 to stored_dfs/df25.csv\n",
      "Saved df26 to stored_dfs/df26.csv\n",
      "Saved df27 to stored_dfs/df27.csv\n",
      "Saved df28 to stored_dfs/df28.csv\n",
      "Saved df29 to stored_dfs/df29.csv\n"
     ]
    }
   ],
   "source": [
    "def save_dataframes_to_csv(df_list):\n",
    "    os.makedirs(\"stored_dfs\", exist_ok=True)  # Create the directory if it doesn't exist\n",
    "    i = 1\n",
    "    for df in df_list:\n",
    "        filename = f\"stored_dfs/df{i}.csv\"\n",
    "        df.to_csv(filename, index=True)\n",
    "        print(f\"Saved df{i} to {filename}\")\n",
    "        i = i + 1\n",
    "\n",
    "save_dataframes_to_csv(cleaned_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ddc8e4-3fcf-43b5-9e38-75e99ffe2b1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
